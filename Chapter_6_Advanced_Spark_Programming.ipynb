{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Chapter 6 - Advanced Spark Programming.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jmbanda/BigDataProgramming_2019/blob/master/Chapter_6_Advanced_Spark_Programming.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GLQhejTr0nXl",
        "colab_type": "text"
      },
      "source": [
        "# Dataframe basics for PySpark"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4vnDcDUZ51VG",
        "colab_type": "text"
      },
      "source": [
        "**Colab only code**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5kkydKPa5viT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!wget -q https://www-us.apache.org/dist/spark/spark-2.4.4/spark-2.4.4-bin-hadoop2.7.tgz\n",
        "!tar xf spark-2.4.4-bin-hadoop2.7.tgz\n",
        "!pip install -q findspark\n",
        "!pip install -q pandas\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-2.4.4-bin-hadoop2.7\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BkZl8pkJ6gr_",
        "colab_type": "text"
      },
      "source": [
        "**Setup Spark Session**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q8F_0FIE58HT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import findspark\n",
        "import pandas as pd #This imports Pandas (we will be using this extensively later)\n",
        "findspark.init()\n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder \\\n",
        "    .master(\"local[*]\") \\\n",
        "    .appName(\"Learning_Spark\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "sc = spark.sparkContext"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_HtyLV7G5DM9",
        "colab_type": "text"
      },
      "source": [
        "Let's start with a subset of the Titanic data on Kaggle and load it into a pandas dataframe, then convert it into a Spark dataframe."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WuQnNr-pwwKp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data1 = {'PassengerId': {0: 1, 1: 2, 2: 3, 3: 4, 4: 5},\n",
        "         'Name': {0: 'Owen', 1: 'Florence', 2: 'Laina', 3: 'Lily', 4: 'William'},\n",
        "         'Sex': {0: 'male', 1: 'female', 2: 'female', 3: 'female', 4: 'male'},\n",
        "         'Survived': {0: 0, 1: 1, 2: 1, 3: 1, 4: 0}}\n",
        "\n",
        "data2 = {'PassengerId': {0: 1, 1: 2, 2: 3, 3: 4, 4: 5},\n",
        "         'Age': {0: 22, 1: 38, 2: 26, 3: 35, 4: 35},\n",
        "         'Fare': {0: 7.3, 1: 71.3, 2: 7.9, 3: 53.1, 4: 8.0},\n",
        "         'Pclass': {0: 3, 1: 1, 2: 3, 3: 1, 4: 3}}\n",
        "\n",
        "df1_pd = pd.DataFrame(data1, columns=data1.keys())\n",
        "df2_pd = pd.DataFrame(data2, columns=data2.keys())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qqXcGg4v6500",
        "colab_type": "text"
      },
      "source": [
        "Let's look at our Panda's dataframe: df1_pd contents"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "brzv7Mg06rP8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df1_pd"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fl8HxvQT7Dba",
        "colab_type": "text"
      },
      "source": [
        "Let's look at the other Panda's dataframe "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3HfCt2747HhL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df2_pd"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZkuD5cjv7M44",
        "colab_type": "text"
      },
      "source": [
        "**We now convert the Panda's dataframe to a Spark dataframe and display its contents**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4jLRySjk7S_g",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df1 = spark.createDataFrame(df1_pd)\n",
        "df2 = spark.createDataFrame(df2_pd)\n",
        "df1.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mW9rGm_b7hQE",
        "colab_type": "text"
      },
      "source": [
        "Let's see the schema"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-lApzWjk7kFv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df1.printSchema()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P4q_CGf68MS9",
        "colab_type": "text"
      },
      "source": [
        "# Basic dataframe transformations\n",
        "\n",
        "![alt text](https://changhsinlee.com/figure/source/2018-03-04-pyspark-dataframe-basics/dataframe-verbs.png)\n",
        "\n",
        "**Select**\n",
        "\n",
        "Takes either a list of column names or an unpacked list."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c2oSJzoi8L6z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cols1 = ['PassengerId', 'Name']\n",
        "df1.select(cols1).show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ljAgPea5AcTm",
        "colab_type": "text"
      },
      "source": [
        "**Filter**\n",
        "\n",
        "Filter with column expression"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9ws-_VGkAhAO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df1.filter(df1.Sex == 'female').show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MS955As_ArwQ",
        "colab_type": "text"
      },
      "source": [
        "Filter with SQL expression. Note the double and single quotes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x_H2IXLIAsnu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df1.filter(\"Sex='male'\").show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NFnHt7z_A4oT",
        "colab_type": "text"
      },
      "source": [
        "**Mutate, or creating new columns**\n",
        "\n",
        "Creating new columns in Spark uses `.withColumn()`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uBYDes7fA8n5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df2.withColumn('AgeTimesFare', df2.Age*df2.Fare).show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dxcL0FWFBM55",
        "colab_type": "text"
      },
      "source": [
        "**Summarize and group by**\n",
        "\n",
        "To summarize or aggregate a dataframe, first you need to convert the dataframe to a *GroupedData* object with `groupby()`, then call aggregate transformations."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P2tpbol1BZqr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "gdf2 = df2.groupby('Pclass')\n",
        "gdf2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P_6G9DVfBe59",
        "colab_type": "text"
      },
      "source": [
        "Note: We take the average of more than one column by passing an unpacked list of column names"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iHHG5K9fBwnN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "avg_cols = ['Age', 'Fare']\n",
        "gdf2.avg(*avg_cols).show() #Note the *, this is unpacking the list!"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tLPXYIZlCGMf",
        "colab_type": "text"
      },
      "source": [
        "Note: To call multiple aggregation functions at once, pass a dictionary"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xrPT9jFPCOPg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "gdf2.agg({'*': 'count', 'Age': 'avg', 'Fare':'sum'}).show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aaGTuKbeD5uw",
        "colab_type": "text"
      },
      "source": [
        "These column names look ugly (at least to me) let's make them read nicely using `toDF()`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_AtfaeMYD_Y7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "(\n",
        "  gdf2.agg({'*': 'count', 'Age': 'avg', 'Fare':'sum'})\n",
        "    .toDF('Pclass', 'counts', 'average_age', 'total_fare')\n",
        "    .show()\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iZwRHY_wEWxO",
        "colab_type": "text"
      },
      "source": [
        "**Arrange (sort)**\n",
        "\n",
        "Use the `.sort()` method to sort the dataframes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "97Ezb7V1Ec5j",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df2.sort('Fare', ascending=False).show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o-KuUvLbEj2g",
        "colab_type": "text"
      },
      "source": [
        "**Joins and unions**\n",
        "\n",
        "There are two ways to combine dataframes --- joins and unions. The idea here is the same as joining and unioning tables in SQL.\n",
        "\n",
        "**Joins**\n",
        "\n",
        "Let's Join the two titanic dataframes by the column *PassengerId*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CbcviaZ8ExL2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df1.join(df2, ['PassengerId']).show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1IWTD5RFE5rj",
        "colab_type": "text"
      },
      "source": [
        "**Nonequi joins**\n",
        "\n",
        "Here is an example of nonequi join. They can be very slow due to skewed data, but this is one operation that Spark can do while Hive can not."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f_G-fXxRFObx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df1.join(df2, df1.PassengerId <= df2.PassengerId).show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5RtUj9maFYKl",
        "colab_type": "text"
      },
      "source": [
        "**Unions**\n",
        "\n",
        "`Union()` returns a dataframe from the union of two dataframes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q51JdORUFggb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df1.union(df1).show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OY_vRQlaFtDC",
        "colab_type": "text"
      },
      "source": [
        "# Going deeper\n",
        "\n",
        "**Explain(), transformations, and actions**\n",
        "\n",
        "When you create a dataframe in PySpark, unlike Python objects, dataframes are lazy evaluated. What it means is that most operations are *transformations* that modify the execution plan on how Spark should handle the data, but the plan is not executed until we call an *action*.\n",
        "\n",
        "For example, if I want to `join` df1 and df2 on the key *PassengerId* as before:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DNt7UUhRGCv_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df1.explain()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CKVGmvX2GDwY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df2.explain()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "80eIdrqRGG-P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dfj1 = df1.join(df2, ['PassengerId'])\n",
        "dfj1.explain()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y4IVNATKGTHG",
        "colab_type": "text"
      },
      "source": [
        "In this case, `join()` is a transformation that laid out a plan for Spark to join the two dataframes, but it wasn't executed unless you call an action, such as `.count()`, that has to go through the actual data defined by df1 and df2 in order to return a Python object (integer)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IosGaMNLGSx0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dfj1.count()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zYBOrt_rGjbQ",
        "colab_type": "text"
      },
      "source": [
        "# Data persistence: cache() and checkpoint()\n",
        "\n",
        "**caching**\n",
        "\n",
        "Proper caching is the key to high performance Spark. \n",
        "\n",
        "\n",
        "A rule of thumb is that: **Cache a dataframe when it is used multiple times in the script.**\n",
        "\n",
        "Keep in mind that it is only cached after the *first action*. If for whatever reason you want to make sure the data is cached before you save the dataframe, then you have to call an action like `.count()`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YCYBr09SG01A",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df1.cache()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pvT7-NFQHAqc",
        "colab_type": "text"
      },
      "source": [
        "This also works as .cache() is an inplace method."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vnh6BgrYHCGD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df1 = df1.cache()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aCIEhVkDHFCV",
        "colab_type": "text"
      },
      "source": [
        "To check if a dataframe is cached, check the storageLevel property."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z7A_fWrpHO1K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df1.storageLevel"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r09Zv5jaHPvi",
        "colab_type": "text"
      },
      "source": [
        "To **un-cache** a dataframe, use `unpersist()`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I3Cgex_6HUo0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df1.unpersist()\n",
        "df1.storageLevel"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T2VuXvv3HvKc",
        "colab_type": "text"
      },
      "source": [
        "**Checkpointing**\n",
        "\n",
        "`checkpoint()` truncates the execution plan and saves the checkpointed dataframe to a temporary location on the disk.\n",
        "\n",
        "It is recommended to do caching before checkpointing so Spark doesn't have to read in the dataframe from disk after it's checkpointed.\n",
        "\n",
        "To use `checkpoint()`, you need to specify the temporary file location to save the datafame to by accessing the sparkContext object from SparkSession."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ek1XKpb6H-q1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sc = spark.sparkContext\n",
        "sc.setCheckpointDir(\"/checkpointdir\") # save to ./checkpointdir"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jc-AO4tfIGqN",
        "colab_type": "text"
      },
      "source": [
        "For example, let's join df1 to itself 3 times:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nQfLm-d5ILbV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df = df1.join(df1, ['PassengerId'])\n",
        "df.join(df1, ['PassengerId']).explain()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J-Ns03yWIPku",
        "colab_type": "text"
      },
      "source": [
        "Let's `checkpoint()` after the first `join` to truncate the plan."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IMWt210yIX7o",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df = df1.join(df1, ['PassengerId']).checkpoint()\n",
        "df.join(df1, ['PassengerId']).explain()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RSSuarwGIm8x",
        "colab_type": "text"
      },
      "source": [
        "**Partitions and repartition()**\n",
        "\n",
        "A common cause of performance problems is having too many partitions. \n",
        "\n",
        "To check the number of partitions, use `.rdd.getNumPartitions()`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UX9P4NXGIkL9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df1.rdd.getNumPartitions()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KHeGR3v9I-8T",
        "colab_type": "text"
      },
      "source": [
        "This dataframe, despite having only 5 rows, has 2 partitions.\n",
        "\n",
        "This is too many, let's repartition to only 1 partition."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dt3l7kmDI-aX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df1_repartitioned = df1.repartition(1)\n",
        "df1_repartitioned.rdd.getNumPartitions()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tmYH_bst4KNs",
        "colab_type": "text"
      },
      "source": [
        "Sources:\n",
        "https://changhsinlee.com/pyspark-dataframe-basics/\n",
        "\n",
        "More about unpacking: \n",
        "https://thispointer.com/python-how-to-unpack-list-tuple-or-dictionary-to-function-arguments-using/"
      ]
    }
  ]
}