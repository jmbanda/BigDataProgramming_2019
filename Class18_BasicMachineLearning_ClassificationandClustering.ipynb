{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Class18-BasicMachineLearning-ClassificationandClustering.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jmbanda/BigDataProgramming_2019/blob/master/Class18_BasicMachineLearning_ClassificationandClustering.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0WAMbHkB9Ycf",
        "colab_type": "text"
      },
      "source": [
        "# Basic Machine Learning - Classification and Clustering"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lSSiKqIU9Wh9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%matplotlib inline\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns; sns.set()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "yNC7e33mViBi",
        "colab_type": "text"
      },
      "source": [
        "## Gaussian Naive Bayes\n",
        "\n",
        "Perhaps the easiest naive Bayes classifier to understand is Gaussian naive Bayes.\n",
        "In this classifier, the assumption is that *data from each label is drawn from a simple Gaussian distribution*.\n",
        "Imagine that you have the following data:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "TTzKvyArViBj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.datasets import make_blobs\n",
        "X, y = make_blobs(100, 2, centers=2, random_state=2, cluster_std=1.5)\n",
        "plt.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='RdBu');"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "deletable": true,
        "editable": true,
        "id": "6SnZjp7BViBm",
        "colab_type": "text"
      },
      "source": [
        "This procedure is implemented in Scikit-Learn's ``sklearn.naive_bayes.GaussianNB`` estimator:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "cxwI5SKaViBm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.naive_bayes import GaussianNB\n",
        "model = GaussianNB()\n",
        "model.fit(X, y);"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "GXBCjQeOViBn",
        "colab_type": "text"
      },
      "source": [
        "Now let's generate some new data and predict the label:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "h-QbcbjFViBo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "rng = np.random.RandomState(0)\n",
        "Xnew = [-6, -14] + [14, 18] * rng.rand(2000, 2)\n",
        "ynew = model.predict(Xnew)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "jPwZBzzeViBp",
        "colab_type": "text"
      },
      "source": [
        "Now we can plot this new data to get an idea of where the decision boundary is:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "iMx2w3PsViBq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='RdBu')\n",
        "lim = plt.axis()\n",
        "plt.scatter(Xnew[:, 0], Xnew[:, 1], c=ynew, s=20, cmap='RdBu', alpha=0.1)\n",
        "plt.axis(lim);"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "j2bzngHsViBr",
        "colab_type": "text"
      },
      "source": [
        "We see a slightly curved boundary in the classifications—in general, the boundary in Gaussian naive Bayes is quadratic.\n",
        "\n",
        "A nice piece of this Bayesian formalism is that it naturally allows for probabilistic classification, which we can compute using the ``predict_proba`` method:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "tnmONpOHViBs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "yprob = model.predict_proba(Xnew)\n",
        "yprob[-8:].round(2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "F3AhSkVfViBt",
        "colab_type": "text"
      },
      "source": [
        "The columns give the posterior probabilities of the first and second label, respectively.\n",
        "If you are looking for estimates of uncertainty in your classification, Bayesian approaches like this can be a useful approach.\n",
        "\n",
        "Of course, the final classification will only be as good as the model assumptions that lead to it, which is why Gaussian naive Bayes often does not produce very good results.\n",
        "Still, in many cases—especially as the number of features becomes large—this assumption is not detrimental enough to prevent Gaussian naive Bayes from being a useful method."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "5dBXnhIgViBu",
        "colab_type": "text"
      },
      "source": [
        "## Multinomial Naive Bayes\n",
        "\n",
        "The Gaussian assumption just described is by no means the only simple assumption that could be used to specify the generative distribution for each label.\n",
        "Another useful example is multinomial naive Bayes, where the features are assumed to be generated from a simple multinomial distribution.\n",
        "The multinomial distribution describes the probability of observing counts among a number of categories, and thus multinomial naive Bayes is most appropriate for features that represent counts or count rates.\n",
        "\n",
        "The idea is precisely the same as before, except that instead of modeling the data distribution with the best-fit Gaussian, we model the data distribuiton with a best-fit multinomial distribution."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "HwHSsiZRViBu",
        "colab_type": "text"
      },
      "source": [
        "### Example: Classifying Text\n",
        "\n",
        "One place where multinomial naive Bayes is often used is in text classification, where the features are related to word counts or frequencies within the documents to be classified.\n",
        "We discussed the extraction of such features from text in [Feature Engineering](05.04-Feature-Engineering.ipynb); here we will use the sparse word count features from the 20 Newsgroups corpus to show how we might classify these short documents into categories.\n",
        "\n",
        "Let's download the data and take a look at the target names:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "KAonIo1YViBu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.datasets import fetch_20newsgroups\n",
        "\n",
        "data = fetch_20newsgroups()\n",
        "data.target_names"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "uwTrNcsIViBw",
        "colab_type": "text"
      },
      "source": [
        "For simplicity here, we will select just a few of these categories, and download the training and testing set:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "y0eWGY9oViBw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "categories = ['talk.religion.misc', 'soc.religion.christian',\n",
        "              'sci.space', 'comp.graphics']\n",
        "train = fetch_20newsgroups(subset='train', categories=categories)\n",
        "test = fetch_20newsgroups(subset='test', categories=categories)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "ppMV6n7gViBy",
        "colab_type": "text"
      },
      "source": [
        "Here is a representative entry from the data:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "vZ4rLyFKViBy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(train.data[5])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "Y09f-32IViB0",
        "colab_type": "text"
      },
      "source": [
        "In order to use this data for machine learning, we need to be able to convert the content of each string into a vector of numbers.\n",
        "For this we will use the TF-IDF (term frequency–inverse document frequency) vectorizer , and create a pipeline that attaches it to a multinomial naive Bayes classifier:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "R6b72OAxViB1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.pipeline import make_pipeline\n",
        "\n",
        "model = make_pipeline(TfidfVectorizer(), MultinomialNB())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "j86mym91ViB3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.fit(train.data, train.target)\n",
        "labels = model.predict(test.data)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "BsjKgkVPViB4",
        "colab_type": "text"
      },
      "source": [
        "Now that we have predicted the labels for the test data, we can evaluate them to learn about the performance of the estimator.\n",
        "For example, here is the confusion matrix between the true and predicted labels for the test data:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "aDcVLOFmViB4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "mat = confusion_matrix(test.target, labels)\n",
        "sns.heatmap(mat.T, square=True, annot=True, fmt='d', cbar=False,\n",
        "            xticklabels=train.target_names, yticklabels=train.target_names)\n",
        "plt.xlabel('true label')\n",
        "plt.ylabel('predicted label');"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "8GPYWaZWViB6",
        "colab_type": "text"
      },
      "source": [
        "Evidently, even this very simple classifier can successfully separate space talk from computer talk, but it gets confused between talk about religion and talk about Christianity.\n",
        "This is perhaps an expected area of confusion!\n",
        "\n",
        "The very cool thing here is that we now have the tools to determine the category for *any* string, using the ``predict()`` method of this pipeline.\n",
        "Here's a quick utility function that will return the prediction for a single string:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "mD1YUsFyViB6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def predict_category(s, train=train, model=model):\n",
        "    pred = model.predict([s])\n",
        "    return train.target_names[pred[0]]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "Wop-gabLViB8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "predict_category('sending a payload to the ISS')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "43rTrzfrViB_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "predict_category('discussing islam vs atheism')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "QV15FU-iViCB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "predict_category('determining the screen resolution')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "Bvt9aChqViCD",
        "colab_type": "text"
      },
      "source": [
        "Remember that this is nothing more sophisticated than a simple probability model for the (weighted) frequency of each word in the string; nevertheless, the result is striking.\n",
        "Even a very naive algorithm, when used carefully and trained on a large set of high-dimensional data, can be surprisingly effective."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s1Sxxy7HsVCC",
        "colab_type": "text"
      },
      "source": [
        "# Decision Trees"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R9g9APp3sOGo",
        "colab_type": "text"
      },
      "source": [
        "## Creating a decision tree\n",
        "\n",
        "Consider the following two-dimensional data, which has one of four class labels:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7GjgHPmOsOGo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.datasets import make_blobs\n",
        "\n",
        "X, y = make_blobs(n_samples=300, centers=4,\n",
        "                  random_state=0, cluster_std=1.0)\n",
        "plt.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='rainbow');"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NGFL24lRsOGr",
        "colab_type": "text"
      },
      "source": [
        "A simple decision tree built on this data will iteratively split the data along one or the other axis according to some quantitative criterion, and at each level assign the label of the new region according to a majority vote of points within it.\n",
        "This figure presents a visualization of the first four levels of a decision tree classifier for this data:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e82_tmMRsOGs",
        "colab_type": "text"
      },
      "source": [
        "This process of fitting a decision tree to our data can be done in Scikit-Learn with the ``DecisionTreeClassifier`` estimator:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "doegcG1osOGt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.tree import DecisionTreeClassifier\n",
        "tree = DecisionTreeClassifier().fit(X, y)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GfROfGXGsOGv",
        "colab_type": "text"
      },
      "source": [
        "Let's write a quick utility function to help us visualize the output of the classifier:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ev1zHz3-sOGv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def visualize_classifier(model, X, y, ax=None, cmap='rainbow'):\n",
        "    ax = ax or plt.gca()\n",
        "    \n",
        "    # Plot the training points\n",
        "    ax.scatter(X[:, 0], X[:, 1], c=y, s=30, cmap=cmap,\n",
        "               clim=(y.min(), y.max()), zorder=3)\n",
        "    ax.axis('tight')\n",
        "    ax.axis('off')\n",
        "    xlim = ax.get_xlim()\n",
        "    ylim = ax.get_ylim()\n",
        "    \n",
        "    # fit the estimator\n",
        "    model.fit(X, y)\n",
        "    xx, yy = np.meshgrid(np.linspace(*xlim, num=200),\n",
        "                         np.linspace(*ylim, num=200))\n",
        "    Z = model.predict(np.c_[xx.ravel(), yy.ravel()]).reshape(xx.shape)\n",
        "\n",
        "    # Create a color plot with the results\n",
        "    n_classes = len(np.unique(y))\n",
        "    contours = ax.contourf(xx, yy, Z, alpha=0.3,\n",
        "                           levels=np.arange(n_classes + 1) - 0.5,\n",
        "                           cmap=cmap,\n",
        "                           zorder=1)\n",
        "\n",
        "    ax.set(xlim=xlim, ylim=ylim)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s71uepRVsOGx",
        "colab_type": "text"
      },
      "source": [
        "Now we can examine what the decision tree classification looks like:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aQgI0wAfsOGx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "visualize_classifier(DecisionTreeClassifier(), X, y)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Beym_ea_sOG5",
        "colab_type": "text"
      },
      "source": [
        "## Ensembles of Estimators: Random Forests\n",
        "\n",
        "This notion—that multiple overfitting estimators can be combined to reduce the effect of this overfitting—is what underlies an ensemble method called *bagging*.\n",
        "Bagging makes use of an ensemble (a grab bag, perhaps) of parallel estimators, each of which over-fits the data, and averages the results to find a better classification.\n",
        "An ensemble of randomized decision trees is known as a *random forest*.\n",
        "\n",
        "This type of bagging classification can be done manually using Scikit-Learn's ``BaggingClassifier`` meta-estimator, as shown here:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SwGOSgZTsOG5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "\n",
        "tree = DecisionTreeClassifier()\n",
        "bag = BaggingClassifier(tree, n_estimators=100, max_samples=0.8,\n",
        "                        random_state=1)\n",
        "\n",
        "bag.fit(X, y)\n",
        "visualize_classifier(bag, X, y)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wZwm5xJxsOG8",
        "colab_type": "text"
      },
      "source": [
        "In this example, we have randomized the data by fitting each estimator with a random subset of 80% of the training points.\n",
        "In practice, decision trees are more effectively randomized by injecting some stochasticity in how the splits are chosen: this way all the data contributes to the fit each time, but the results of the fit still have the desired randomness.\n",
        "For example, when determining which feature to split on, the randomized tree might select from among the top several features.\n",
        "You can read more technical details about these randomization strategies in the [Scikit-Learn documentation](http://scikit-learn.org/stable/modules/ensemble.html#forest) and references within.\n",
        "\n",
        "In Scikit-Learn, such an optimized ensemble of randomized decision trees is implemented in the ``RandomForestClassifier`` estimator, which takes care of all the randomization automatically.\n",
        "All you need to do is select a number of estimators, and it will very quickly (in parallel, if desired) fit the ensemble of trees:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RNRfnzS4sOG8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "model = RandomForestClassifier(n_estimators=100, random_state=0)\n",
        "visualize_classifier(model, X, y);"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W66Kt4GusOG-",
        "colab_type": "text"
      },
      "source": [
        "We see that by averaging over 100 randomly perturbed models, we end up with an overall model that is much closer to our intuition about how the parameter space should be split."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UjyHR9Y4sOHD",
        "colab_type": "text"
      },
      "source": [
        "## Example: Random Forest for Classifying Digits\n",
        "\n",
        "Earlier we took a quick look at the hand-written digits data.\n",
        "Let's use that again here to see how the random forest classifier can be used in this context."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xZ9ba5aBsOHD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.datasets import load_digits\n",
        "digits = load_digits()\n",
        "digits.keys()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pCqI68FvsOHG",
        "colab_type": "text"
      },
      "source": [
        "To remind us what we're looking at, we'll visualize the first few data points:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5AA0ldehsOHH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# set up the figure\n",
        "fig = plt.figure(figsize=(6, 6))  # figure size in inches\n",
        "fig.subplots_adjust(left=0, right=1, bottom=0, top=1, hspace=0.05, wspace=0.05)\n",
        "\n",
        "# plot the digits: each image is 8x8 pixels\n",
        "for i in range(64):\n",
        "    ax = fig.add_subplot(8, 8, i + 1, xticks=[], yticks=[])\n",
        "    ax.imshow(digits.images[i], cmap=plt.cm.binary, interpolation='nearest')\n",
        "    \n",
        "    # label the image with the target value\n",
        "    ax.text(0, 7, str(digits.target[i]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8iZvCYsZsOHJ",
        "colab_type": "text"
      },
      "source": [
        "We can quickly classify the digits using a random forest as follows:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f9rVxa9bsOHJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "Xtrain, Xtest, ytrain, ytest = train_test_split(digits.data, digits.target,\n",
        "                                                random_state=0)\n",
        "model = RandomForestClassifier(n_estimators=1000)\n",
        "model.fit(Xtrain, ytrain)\n",
        "ypred = model.predict(Xtest)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5-IcQm-msOHK",
        "colab_type": "text"
      },
      "source": [
        "We can take a look at the classification report for this classifier:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kf7s197KsOHL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn import metrics\n",
        "print(metrics.classification_report(ypred, ytest))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PGFiSDNasOHN",
        "colab_type": "text"
      },
      "source": [
        "And for good measure, plot the confusion matrix:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0bw89pKlsOHN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "mat = confusion_matrix(ytest, ypred)\n",
        "sns.heatmap(mat.T, square=True, annot=True, fmt='d', cbar=False)\n",
        "plt.xlabel('true label')\n",
        "plt.ylabel('predicted label');"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4joDHGgNsOHP",
        "colab_type": "text"
      },
      "source": [
        "We find that a simple, untuned random forest results in a very accurate classification of the digits data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3QbVO58E-bGb",
        "colab_type": "text"
      },
      "source": [
        "# Clustering"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P_RgApVL9IPO",
        "colab_type": "text"
      },
      "source": [
        "## Looking for clusters visually"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EdTmVLbr9IPR",
        "colab_type": "text"
      },
      "source": [
        "You are given an array `points` of size 300x2, where each row gives the (x, y) co-ordinates of a point on a map.  Make a scatter plot of these points, and use the scatter plot to guess how many clusters there are."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rbDbhtbwzrsm",
        "colab_type": "text"
      },
      "source": [
        "Colab only code:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LmLs3OLFztMg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import files\n",
        "files.upload()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dVWwEfSiztbQ",
        "colab_type": "text"
      },
      "source": [
        "End of colab only code"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KaOIqeBA9IPR",
        "colab_type": "text"
      },
      "source": [
        "**Step 1:** Load the dataset clustering1.csv *(found in iCollege)*."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "exercise": false,
        "id": "JtXrbql19IPS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv('clustering1.csv')\n",
        "points = df.values"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ug1dHhBA9IPU",
        "colab_type": "text"
      },
      "source": [
        "**Step 2:** Import PyPlot"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "278Ye-AV9IPV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xlg2ZO8G9IPW",
        "colab_type": "text"
      },
      "source": [
        "**Step 3:** Create an array called `xs` that contains the values of `points[:,0]` - that is, column `0` of `points`:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3a2Gk9rj9IPX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "xs = points[:,0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jQ5t7dUa9IPY",
        "colab_type": "text"
      },
      "source": [
        "**Step 3:** Create an array called `ys` that contains the values of `points[:,1]` - that is, column `1` of `points`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SdGo4enL9IPZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ys = points[:,1]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0GCD0fg59IPa",
        "colab_type": "text"
      },
      "source": [
        "**Step 4:** Make a scatter plot by passing `xs` and `ys` to the `plt.scatter()` function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "siRJRz4A9IPb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.scatter(xs, ys)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x14eDvVTyenC",
        "colab_type": "text"
      },
      "source": [
        "So what can we see here? How many clusters do we have?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c_oPeOAy-tXW",
        "colab_type": "text"
      },
      "source": [
        "## Clustering 2D points"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CksqkEfh-tXY",
        "colab_type": "text"
      },
      "source": [
        "From the scatter plot of the previous section, you saw that the points seem to separate into 3 clusters.  Now create a KMeans model to find 3 clusters, and fit it to the data points from the previous exercise.  After the model has been fit, obtain the cluster labels for points, and also for some new points using the `.predict()` method.\n",
        "\n",
        "You are given the array `points` from the previous exercise, and also an array `new_points`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pd1xt0Uj-tXZ",
        "colab_type": "text"
      },
      "source": [
        "**Step 1:** Load the dataset (clustering1.csv and clustering2.csv)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "exercise": false,
        "id": "nabBr3n9-tXa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv('clustering1.csv')\n",
        "points = df.values\n",
        "\n",
        "new_df = pd.read_csv('clustering2.csv')\n",
        "new_points = new_df.values"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ji8ihxRI-tXc",
        "colab_type": "text"
      },
      "source": [
        "**Step 2:** Import `KMeans` from `sklearn.cluster`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2DAz8O2p-tXd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.cluster import KMeans"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aPpGGWdf-tXf",
        "colab_type": "text"
      },
      "source": [
        "**Step 3:** Using `KMeans()`, create a `KMeans` instance called `model` to find `3` clusters. To specify the number of clusters, use the `n_clusters` keyword argument"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H24bPkxE-tXf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = KMeans(n_clusters=3)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ktkrM3zc-tXh",
        "colab_type": "text"
      },
      "source": [
        "**Step 4:** Use the `.fit()` method of `model` to fit the model to the array of points `points`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O6JvasqJ-tXh",
        "colab_type": "code",
        "outputId": "3efad233-24aa-4f96-9ee2-f0c2ffbd7197",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "model.fit(points)"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "KMeans(algorithm='auto', copy_x=True, init='k-means++', max_iter=300,\n",
              "       n_clusters=3, n_init=10, n_jobs=None, precompute_distances='auto',\n",
              "       random_state=None, tol=0.0001, verbose=0)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RcNrXDXA-tXk",
        "colab_type": "text"
      },
      "source": [
        "**Step 5:** Use the `.predict()` method of `model` to predict the cluster labels of `points`, assigning the result to `labels`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XvHiFRmn-tXk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "labels = model.predict(points)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WPje3pZH-tXm",
        "colab_type": "text"
      },
      "source": [
        "**Step 6:** Print out the labels, and have a look at them!  _(In the next section, I'll show you how to visualise this clustering better.)_"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I9UZ7j0D-tXo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(labels)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R1w2xP-W-tXp",
        "colab_type": "text"
      },
      "source": [
        "**Step 7:** Use the `.predict()` method of `model` to predict the cluster labels of `new_points`, assigning the result to `new_labels`.  Notice that KMeans can assign previously unseen points to the clusters it has already found!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vBbn7tEE-tXq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "new_labels = model.predict(new_points)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4DjeOSTz_Mr6",
        "colab_type": "text"
      },
      "source": [
        "## Inspect your clustering"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FR6_IkMc_Mr9",
        "colab_type": "text"
      },
      "source": [
        "Let's now inspect the clustering you performed in the previous exercise!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cDYdamFy_Mr-",
        "colab_type": "text"
      },
      "source": [
        "**Step 1:** Load the dataset (clustering1.csv)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "exercise": false,
        "id": "bxp-LCDV_Mr_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv('clustering1.csv')\n",
        "points = df.values"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QjBoLH-A_MsB",
        "colab_type": "text"
      },
      "source": [
        "**Step 2:** Run your solution to the previous section."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "exercise": false,
        "id": "lpbg9csM_MsC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.cluster import KMeans\n",
        "\n",
        "model = KMeans(n_clusters=3)\n",
        "model.fit(points)\n",
        "labels = model.predict(points)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-L6fTKr1_MsE",
        "colab_type": "text"
      },
      "source": [
        "**Step 3:** Import `matplotlib.pyplot` as `plt`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HxzuwbEd_MsE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IMMjcc5F_MsG",
        "colab_type": "text"
      },
      "source": [
        "**Step 4:** Assign column `0` of `points` to `xs`, and column `1` of `points` to `ys`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wqw3hM8H_MsG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "xs = points[:,0]\n",
        "ys = points[:,1]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "opAJmtER_MsI",
        "colab_type": "text"
      },
      "source": [
        "**Step 5:** Make a scatter plot of `xs` and `ys`, specifying the `c=labels` keyword arguments to color the points by their cluster label.  You'll see that KMeans has done a good job of identifying the clusters!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XD6k0Uor_MsI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.scatter(xs, ys, c=labels)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4TnZ7Hwm_MsL",
        "colab_type": "text"
      },
      "source": [
        "**This is great**, but let's go one step further, and add the cluster centres (the \"centroids\") to the scatter plot."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ihp5YbYV_MsL",
        "colab_type": "text"
      },
      "source": [
        "**Step 6:** Obtain the coordinates of the centroids using the `.cluster_centers_` attribute of `model`.  Assign them to `centroids`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yKJVdRHl_MsL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "centroids = model.cluster_centers_"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Du5OzVyy_MsP",
        "colab_type": "text"
      },
      "source": [
        "**Step 7:** Assign column `0` of `centroids` to `centroids_x`, and column `1` of `centroids` to `centroids_y`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8WSUdYyh_MsP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "centroids_x = centroids[:,0]\n",
        "centroids_y = centroids[:,1]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qDGgKtB7_MsQ",
        "colab_type": "text"
      },
      "source": [
        "**Step 8:** In a single cell, create two scatter plots (this will show the two on top of one another).  Call `plt.show()` just once, at the end.\n",
        "\n",
        "Firstly, the make the scatter plot you made above.  Secondly, make a scatter plot of `centroids_x` and `centroids_y`, using `'X'` (a cross) as a marker by specifying the `marker` parameter. Set the size of the markers to be `200` using `s=200`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2PBbQVNE_MsR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.scatter(xs, ys, c=labels)\n",
        "plt.scatter(centroids_x, centroids_y, marker='X', s=200)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OWfSv_Ci_MsS",
        "colab_type": "text"
      },
      "source": [
        "**Great work!** The centroids are important because they are what enables KMeans to assign new, previously unseen points to the existing clusters."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xs9M7OHPJgiu",
        "colab_type": "text"
      },
      "source": [
        "## Example: How many clusters of grain?\n",
        "\n",
        "You are given a dataset of the measurements of samples of grain.  What's a good number of clusters?\n",
        "\n",
        "This dataset was obtained from the [UCI](https://archive.ics.uci.edu/ml/datasets/seeds)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CiMMSrZzJgix",
        "colab_type": "text"
      },
      "source": [
        "**Step 1:** Load the dataset (seeds.csv)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "exercise": false,
        "id": "2aD22cF2Jgiy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "seeds_df = pd.read_csv('seeds.csv')\n",
        "# forget about the grain variety for the moment - we'll use this later\n",
        "del seeds_df['grain_variety']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7WFiPrTcJgi0",
        "colab_type": "text"
      },
      "source": [
        "**Step 2:** Display the DataFrame to inspect the data.  Notice that there are 7 columns - so each grain sample (row) is a point in 7D space!  Scatter plots can't help us here."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k5Ybi27KJgi0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "seeds_df.head(10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KV0vuUZUJgi3",
        "colab_type": "text"
      },
      "source": [
        "**Step 3:** Extract the measurements from the DataFrame using its `.values` attribute:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Ei6Rd1zJgi3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "samples = seeds_df.values"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gSJ3iO5yJgi5",
        "colab_type": "text"
      },
      "source": [
        "**Step 4:**  Measure the quality of clusterings with different numbers of clusters using the\n",
        "inertia.  For each of the given values of `k`, perform the following steps:\n",
        "\n",
        "  - Create a `KMeans` instance called `model` with `k` clusters.\n",
        "  - Fit the model to the grain data `samples`.\n",
        "  - Append the value of the `inertia_` attribute of `model` to the list `inertias`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EcdV713AJgi6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.cluster import KMeans\n",
        "\n",
        "ks = range(1, 6)\n",
        "inertias = []\n",
        "\n",
        "for k in ks:\n",
        "    # Create a KMeans instance with k clusters: model\n",
        "    model = KMeans(n_clusters=k)\n",
        "\n",
        "    # Fit model to samples\n",
        "    model.fit(samples)\n",
        "\n",
        "    # Append the inertia to the list of inertias\n",
        "    inertias.append(model.inertia_)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "INM5zKtrJgi7",
        "colab_type": "text"
      },
      "source": [
        "**Step 5:**  Plot the inertia to see which number of clusters is best. Remember: lower numbers are better!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tcqLE1BYJgi8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Plot ks vs inertias\n",
        "plt.plot(ks, inertias, '-o')\n",
        "plt.xlabel('number of clusters, k')\n",
        "plt.ylabel('inertia')\n",
        "plt.xticks(ks)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bDcZK-zoJgi9",
        "colab_type": "text"
      },
      "source": [
        "**Excellent work!** You can see from the graph that 3 is a good number of clusters, since these are points where the inertia begins to decrease more slowly."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "coj-H8JLJvKm",
        "colab_type": "text"
      },
      "source": [
        "## Exercise: Scaling fish data for clustering\n",
        "\n",
        "You are given an array `samples` giving measurements of fish.  Each row represents asingle fish.  The measurements, such as weight in grams, length in centimeters, and the percentage ratio of height to length, have very different scales.  In order to cluster this data effectively, you'll need to standardize these features first.  In this exercise, you'll build a pipeline to standardize and cluster the data.\n",
        "\n",
        "This great dataset was derived from the one [here](http://svitsrv25.epfl.ch/R-doc/library/rrcov/html/fish.html), where you can see a description of each measurement."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "exercise": false,
        "id": "AZ-PAr3lJvKp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv('fish.csv')\n",
        "species = list(df['species'])\n",
        "\n",
        "# forget the species column for now - we'll use it later!\n",
        "del df['species']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1PRT8LO5JvKr",
        "colab_type": "text"
      },
      "source": [
        "**Step 2:** Call `df.head()` to inspect the dataset:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PO-jnnInJvKr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F5aALTisJvKt",
        "colab_type": "text"
      },
      "source": [
        "**Step 3:** Extract all the measurements as a 2D NumPy array, assigning to `samples` (hint: use the `.values` attribute of `df`)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6jaobQqMJvKu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "samples = df.values"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qVDDave4JvKw",
        "colab_type": "text"
      },
      "source": [
        "**Step 4:** Perform the necessary imports:\n",
        "\n",
        "- `make_pipeline` from `sklearn.pipeline`.\n",
        "- `StandardScaler` from `sklearn.preprocessing`.\n",
        "- `KMeans` from `sklearn.cluster`.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P4v3eHo4JvKx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.cluster import KMeans"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FSC3EUJxJvKz",
        "colab_type": "text"
      },
      "source": [
        "**Step 5:** Create an instance of `StandardScaler` called `scaler`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tXzMCoAyJvK0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "scaler = StandardScaler()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lngUJr3VJvK1",
        "colab_type": "text"
      },
      "source": [
        "**Step 6:** Create an instance of `KMeans` with `4` clusters called `kmeans`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kVGVUUnpJvK2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "kmeans = KMeans(n_clusters=4)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Ftp4WbvJvK3",
        "colab_type": "text"
      },
      "source": [
        "**Step 7:** Create a pipeline called `pipeline` that chains `scaler` and `kmeans`. To do this, you just need to pass them in as arguments to `make_pipeline()`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GcAVVSyKJvK4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pipeline = make_pipeline(scaler, kmeans)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-rLM7z_7JvK5",
        "colab_type": "text"
      },
      "source": [
        "**Great job!** Now you're all set to transform the fish measurements and perform the clustering.  Let's get to it in the next exercise!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gBgToyd1J6rB",
        "colab_type": "text"
      },
      "source": [
        "**Step 8:** Fit the pipeline to the fish measurements `samples`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xaoXK47nJ6rB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pipeline.fit(samples)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ip2oLPhNJ6rE",
        "colab_type": "text"
      },
      "source": [
        "**Step 9:** Obtain the cluster labels for `samples` by using the `.predict()` method of `pipeline`, assigning the result to `labels`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s1W5IRlZJ6rE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "labels = pipeline.predict(samples)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fZpRBx2dJ6rG",
        "colab_type": "text"
      },
      "source": [
        "**Step 10:** Using `pd.DataFrame()`, create a DataFrame `df` with two columns named `'labels'` and `'species'`, using `labels` and `species`, respectively, for the column values."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ta_cHT6KJ6rG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df = pd.DataFrame({'labels': labels, 'species': species})"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J9KUqIHcJ6rH",
        "colab_type": "text"
      },
      "source": [
        "**Step 11:** Using `pd.crosstab()`, create a cross-tabulation `ct` of `df['labels']` and `df['species']`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OculYsOpJ6rI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ct = pd.crosstab(df['labels'], df['species'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3GpGeLSHJ6rK",
        "colab_type": "text"
      },
      "source": [
        "**Step 12:** Display your cross-tabulation, and check out how good your clustering is!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pL8PlEg7J6rK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ct"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "enVI3f089czV",
        "colab_type": "text"
      },
      "source": [
        "Sources:\n",
        "\n",
        "https://jakevdp.github.io/PythonDataScienceHandbook/05.05-naive-bayes.html\n",
        "\n",
        "https://github.com/benjaminwilson/python-clustering-exercises"
      ]
    }
  ]
}