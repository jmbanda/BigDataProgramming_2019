{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.5.1"
    },
    "colab": {
      "name": "Class21-Basic Data Mining Using Python.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jmbanda/BigDataProgramming_2019/blob/master/Class21_Basic_Data_Mining_Using_Python.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lOdgef9zOwYV",
        "colab_type": "text"
      },
      "source": [
        "# Association Rule Mining with Apriori Algorithm"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SJ2ME5rrhD7p",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install apyori  #This instals the Apyori package for using the Association Mining Apriori algorithm\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "from apyori import apriori"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8cmltMCyhVF3",
        "colab_type": "text"
      },
      "source": [
        "Importing the Dataset\n",
        "Now let's import the dataset and see what we're working with. Download the store_data.csv dataset from iCollege."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Xw7EEhSkhh_e",
        "colab": {}
      },
      "source": [
        "from google.colab import files\n",
        "files.upload()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3ekHu5-fheLX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "store_data = pd.read_csv('store_data.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t7IcIb_ghpNZ",
        "colab_type": "text"
      },
      "source": [
        "Let's call the head() function to see how the dataset looks:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Un923mavhsL5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "store_data.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ANMDy-8uh2UF",
        "colab_type": "text"
      },
      "source": [
        "If you carefully look at the data, we can see that the header is actually the first transaction. Each row corresponds to a transaction and each column corresponds to an item purchased in that specific transaction. The NaN tells us that the item represented by the column was not purchased in that specific transaction.\n",
        "\n",
        "In this dataset there is no header row. But by default, pd.read_csv function treats first row as header. To get rid of this problem, add header=None option to pd.read_csv function, as shown below:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5YKuVMb8h6S7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "store_data = pd.read_csv('store_data.csv', header=None)\n",
        "store_data.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "js_n2gR-iDKh",
        "colab_type": "text"
      },
      "source": [
        "## Data Proprocessing\n",
        "\n",
        "The Apriori library we are going to use requires our dataset to be in the form of a list of lists, where the whole dataset is a big list and each transaction in the dataset is an inner list within the outer big list. Currently we have data in the form of a pandas dataframe. To convert our pandas dataframe into a list of lists, execute the following script:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RwJwtADyiHkM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "records = []\n",
        "for i in range(0, 7501):\n",
        "    records.append([str(store_data.values[i,j]) for j in range(0, 20)])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GBr6pd8JiNtV",
        "colab_type": "text"
      },
      "source": [
        "## Applying Apriori\n",
        "\n",
        "The next step is to apply the Apriori algorithm on the dataset. To do so, we can use the apriori class that we imported from the apyori library.\n",
        "\n",
        "The apriori class requires some parameter values to work. The first parameter is the list of list that you want to extract rules from. The second parameter is the min_support parameter. This parameter is used to select the items with support values greater than the value specified by the parameter. Next, the min_confidence parameter filters those rules that have confidence greater than the confidence threshold specified by the parameter. Similarly, the min_lift parameter specifies the minimum lift value for the short listed rules. Finally, the min_length parameter specifies the minimum number of items that you want in your rules.\n",
        "\n",
        "Let's suppose that we want rules for only those items that are purchased at least 5 times a day, or 7 x 5 = 35 times in one week, since our dataset is for a one-week time period. The support for those items can be calculated as 35/7500 = 0.0045. The minimum confidence for the rules is 20% or 0.2. Similarly, we specify the value for lift as 3 and finally min_length is 2 since we want at least two products in our rules. These values are mostly just arbitrarily chosen, so you can play with these values and see what difference it makes in the rules you get back out."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bkDTGvlniSIp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "association_rules = apriori(records, min_support=0.0045, min_confidence=0.2, min_lift=3, min_length=2)\n",
        "association_results = list(association_rules)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L1MTqGwMinb4",
        "colab_type": "text"
      },
      "source": [
        "In the second line here we convert the rules found by the apriori class into a list since it is easier to view the results in this form.\n",
        "\n",
        "## Viewing the Results\n",
        "\n",
        "Let's first find the total number of rules mined by the apriori class. Execute the following script:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "laiaMEDNioOO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(len(association_results))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b2e-JmmJi9G-",
        "colab_type": "text"
      },
      "source": [
        "The script above should return 48. Each item corresponds to one rule.\n",
        "\n",
        "Let's print the first item in the association_rules list to see the first rule. Execute the following script:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hgUQhk3bjAb4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(association_results[0])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IdGdmSExjI5k",
        "colab_type": "text"
      },
      "source": [
        "The first item in the list is a list itself containing three items. The first item of the list shows the grocery items in the rule.\n",
        "\n",
        "For instance from the first item, we can see that light cream and chicken are commonly bought together. This makes sense since people who purchase light cream are careful about what they eat hence they are more likely to buy chicken i.e. white meat instead of red meat i.e. beef. Or this could mean that light cream is commonly used in recipes for chicken.\n",
        "\n",
        "The support value for the first rule is 0.0045. This number is calculated by dividing the number of transactions containing light cream divided by total number of transactions. The confidence level for the rule is 0.2905 which shows that out of all the transactions that contain light cream, 29.05% of the transactions also contain chicken. Finally, the lift of 4.84 tells us that chicken is 4.84 times more likely to be bought by the customers who buy light cream compared to the default likelihood of the sale of chicken.\n",
        "\n",
        "The following script displays the rule, the support, the confidence, and lift for each rule in a more clear way:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "56UPdDoOjMfM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for item in association_results:\n",
        "\n",
        "    # first index of the inner list\n",
        "    # Contains base item and add item\n",
        "    pair = item[0] \n",
        "    items = [x for x in pair]\n",
        "    print(\"Rule: \" + items[0] + \" -> \" + items[1])\n",
        "\n",
        "    #second index of the inner list\n",
        "    print(\"Support: \" + str(item[1]))\n",
        "\n",
        "    #third index of the list located at 0th\n",
        "    #of the third index of the inner list\n",
        "\n",
        "    print(\"Confidence: \" + str(item[2][0][2]))\n",
        "    print(\"Lift: \" + str(item[2][0][3]))\n",
        "    print(\"=====================================\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VYIJQqLajaac",
        "colab_type": "text"
      },
      "source": [
        "We have already discussed the first rule. Let's now discuss the second rule. The second rule states that mushroom cream sauce and escalope are bought frequently. The support for mushroom cream sauce is 0.0057. The confidence for this rule is 0.3006 which means that out of all the transactions containing mushroom, 30.06% of the transactions are likely to contain escalope as well. Finally, lift of 3.79 shows that the escalope is 3.79 more likely to be bought by the customers that buy mushroom cream sauce, compared to its default sale."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_chLUblgOqj-",
        "colab_type": "text"
      },
      "source": [
        "# kNN - k-Nearest Neighbors"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8SCTpdiFb2pL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from scipy import stats\n",
        "%matplotlib inline"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KP8qHFfLcITm",
        "colab_type": "text"
      },
      "source": [
        "## Read in the dataset\n",
        "\n",
        "We will use the pandas .read_csv() method to read in the dataset (knnDataset.csv from iCollege). Then we will use the .head() method to observe the first few rows of the data, to understand the information better. In our case, the feature(column) headers tell us pretty little. This is fine because we are merely trying to gain insight via classifying new data points by referencing it’s neighboring elements."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rbDbhtbwzrsm",
        "colab_type": "text"
      },
      "source": [
        "Colab only code:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LmLs3OLFztMg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import files\n",
        "files.upload()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ISvB-Gl_cV7C",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Use pandas .read_csv() method to read in classified dataset\n",
        "# index_col -> argument assigns the index to a particular column\n",
        "df = pd.read_csv('knnDataset.csv', index_col=0)\n",
        "# Use the .head() method to display the first few rows\n",
        "df.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ocCPiIcmcvvS",
        "colab_type": "text"
      },
      "source": [
        "## Standardize (normalize) the data scale to prep for KNN algorithm\n",
        "\n",
        "Because the distance between pairs of points plays a critical part on the classification, it is necessary to normalize the data to minimize this. This will generate an array of values. Again, KNN depends on the distance between each feature."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s_02DtTidA0N",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Import module to standardize the scale\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "# Create instance (i.e. object) of the standard scaler\n",
        "scaler = StandardScaler()\n",
        "# Fit the object to all the data except the Target Class\n",
        "# use the .drop() method to gather all features except Target Class\n",
        "# axis -> argument refers to columns; a 0 would represent rows\n",
        "scaler.fit(df.drop('TARGET CLASS', axis=1))\n",
        "\n",
        "# Use scaler object to conduct a transforms\n",
        "scaled_features = scaler.transform(df.drop('TARGET CLASS',axis=1))\n",
        "# Review the array of values generated from the scaled features process\n",
        "scaled_features"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G6syPA32dMLm",
        "colab_type": "text"
      },
      "source": [
        "Here we have the normalized dataset, minus the target column"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VbidsXXidRa2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_feat = pd.DataFrame(scaled_features, columns=df.columns[:-1])\n",
        "df_feat.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P_y3ZmyFdmOf",
        "colab_type": "text"
      },
      "source": [
        "## Split the normalized data into training and test sets\n",
        "\n",
        "This step is required to prepare us for the fitting (i.e. training) the model later. The “X” variable is a collection of all the features. The “y” variable is the target label which specifies the classification of 1 or 0 based. Our goal will be to identify which category the new data point should fall into."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X-8MgjwWdzzo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Import module to split the data\n",
        "from sklearn.model_selection import train_test_split\n",
        "# Set the X and ys\n",
        "X = df_feat\n",
        "y = df['TARGET CLASS']\n",
        "# Use the train_test_split() method to split the data into respective sets\n",
        "# test_size -> argument refers to the size of the test subset\n",
        "# random_state -> argument ensures guarantee that the output of Run \n",
        "# 1 will be equal to the output of Run 2, i.e. your split will be always the same\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S4k5eEIQd4tu",
        "colab_type": "text"
      },
      "source": [
        "This allows for use to train our model on the training set and evaluate the built model against the test set to identify errors.\n",
        "\n",
        "## Create and Train the Model\n",
        "\n",
        "Here we create a KNN Object and use the .fit() method to train the model. Upon completion of the model we should receive confirmation that the training has been completed."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G1DUP5LDeCT6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Import module for KNN\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "# Create KNN instance\n",
        "# n_neighbors -> argument identifies the amount of neighbors used to ID classification\n",
        "knn = KNeighborsClassifier(n_neighbors=1)\n",
        "# Fit (i.e. traing) the model\n",
        "knn.fit(X_train, y_train)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UisOhFsieKsC",
        "colab_type": "text"
      },
      "source": [
        "## Make Predictions\n",
        "\n",
        "Here we review where our model was accurate and where it misclassified elements."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hD_9M8PHeQbs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Use the .predict() method to make predictions from the X_test subset\n",
        "pred = knn.predict(X_test)\n",
        "# Review the predictions\n",
        "pred"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VwQXzebIeTum",
        "colab_type": "text"
      },
      "source": [
        "## Evaluate the predictions\n",
        "\n",
        "Evaluate the Model by reviewing the classification report or confusion matrix. By reviewing these tables, we are able to evaluate how accurate our model is with new values."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-UhpQ36XeY12",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Import classification report and confusion matrix to evaluate predictions\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "# Print out classification report and confusion matrix\n",
        "print(classification_report(y_test, pred))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rnr4husUegiN",
        "colab_type": "text"
      },
      "source": [
        "## Confusion Matrix"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "INL9xO5Kehzk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Print out confusion matrix\n",
        "cmat = confusion_matrix(y_test, pred)\n",
        "#print(cmat)\n",
        "print('TP - True Negative {}'.format(cmat[0,0]))\n",
        "print('FP - False Positive {}'.format(cmat[0,1]))\n",
        "print('FN - False Negative {}'.format(cmat[1,0]))\n",
        "print('TP - True Positive {}'.format(cmat[1,1]))\n",
        "print('Accuracy Rate: {}'.format(np.divide(np.sum([cmat[0,0],cmat[1,1]]),np.sum(cmat))))\n",
        "print('Misclassification Rate: {}'.format(np.divide(np.sum([cmat[0,1],cmat[1,0]]),np.sum(cmat))))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m8dfjqdlex9P",
        "colab_type": "text"
      },
      "source": [
        "## Evaluate alternative K-values for better predictions\n",
        "\n",
        "To simplify the process of evaluating multiple cases of k-values, we create a function to derive the error using the average where our predictions were not equal to the test values."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AO5-qqmce2u_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Generate function to add error rates of KNN with various k-values\n",
        "# error_rate -> empty list to gather error rates at various k-values\n",
        "# for loop -> loops through k values 1 to 39\n",
        "# knn -> creates instance of KNeighborsClassifier with various k\n",
        "# knn.fit -> trains the model\n",
        "# pred_i -> conducts predictions from model on test subset\n",
        "# error_rate.append -> adds error rate of model with various k-value, using the average where prediction not\n",
        "# equal to the test values\n",
        "error_rate = []\n",
        "for i in range(1,40):\n",
        "    \n",
        "    knn = KNeighborsClassifier(n_neighbors=i)\n",
        "    knn.fit(X_train, y_train)\n",
        "    pred_i = knn.predict(X_test)\n",
        "    error_rate.append(np.mean(pred_i != y_test))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2jOKuu_ye58T",
        "colab_type": "text"
      },
      "source": [
        "## Plot Error Rate"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-qXtL52je8uD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Configure and plot error rate over k values\n",
        "plt.figure(figsize=(10,4))\n",
        "plt.plot(range(1,40), error_rate, color='blue', linestyle='dashed', marker='o', markerfacecolor='red', markersize=10)\n",
        "plt.title('Error Rate vs. K-Values')\n",
        "plt.xlabel('K-Values')\n",
        "plt.ylabel('Error Rate')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Csypu5V-fE8j",
        "colab_type": "text"
      },
      "source": [
        "Here we see that the error rate continues to decrease as we increase the k-value. A picture tells a thousand words. Or at least here, we are able to understand what value of k leads to an optimal model. The k-value of 15 seems to give a decent error rate without too much noise, as we see with k-values of 25 and larger."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FpW_kaOhfMgw",
        "colab_type": "text"
      },
      "source": [
        "## Adjust K value per error rate evaluations\n",
        "\n",
        "This is just fine tuning our model to increase accuracy. We will need to retrain our model with the new k-value."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P3vI_Tc2fRy3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Retrain model using optimal k-value\n",
        "knn = KNeighborsClassifier(n_neighbors=15)\n",
        "knn.fit(X_train, y_train)\n",
        "pred = knn.predict(X_test)\n",
        "# Print out classification report and confusion matrix\n",
        "print(classification_report(y_test, pred))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hzj7Z_HWfYnP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Print out confusion matrix\n",
        "cmat = confusion_matrix(y_test, pred)\n",
        "#print(cmat)\n",
        "print('TP - True Negative {}'.format(cmat[0,0]))\n",
        "print('FP - False Positive {}'.format(cmat[0,1]))\n",
        "print('FN - False Negative {}'.format(cmat[1,0]))\n",
        "print('TP - True Positive {}'.format(cmat[1,1]))\n",
        "print('Accuracy Rate: {}'.format(np.divide(np.sum([cmat[0,0],cmat[1,1]]),np.sum(cmat))))\n",
        "print('Misclassification Rate: {}'.format(np.divide(np.sum([cmat[0,1],cmat[1,0]]),np.sum(cmat))))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9JqJg3QjJcAF",
        "colab_type": "text"
      },
      "source": [
        "# Support Vector Machines"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "65iJxmHeJcAG",
        "colab_type": "text"
      },
      "source": [
        "Support vector machines (SVMs) are a particularly powerful and flexible class of supervised algorithms for both classification and regression.\n",
        "\n",
        "We begin with the standard imports:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UkJpyh1hJcAH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%matplotlib inline\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy import stats\n",
        "\n",
        "# use seaborn plotting defaults\n",
        "import seaborn as sns; sns.set()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RC1vil7nJcAM",
        "colab_type": "text"
      },
      "source": [
        "## Motivating Support Vector Machines"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KcXXyGR0JcAO",
        "colab_type": "text"
      },
      "source": [
        "In our disussion of Bayesian classification, we learned a simple model describing the distribution of each underlying class, and used these generative models to probabilistically determine labels for new points.\n",
        "That was an example of *generative classification*; here we will consider instead *discriminative classification*: rather than modeling each class, we simply find a line or curve (in two dimensions) or manifold (in multiple dimensions) that divides the classes from each other.\n",
        "\n",
        "As an example of this, consider the simple case of a classification task, in which the two classes of points are well separated:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CXPqHsDqJcAR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.datasets.samples_generator import make_blobs\n",
        "X, y = make_blobs(n_samples=50, centers=2,\n",
        "                  random_state=0, cluster_std=0.60)\n",
        "plt.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='autumn');"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5F5-u-elJcAW",
        "colab_type": "text"
      },
      "source": [
        "A linear discriminative classifier would attempt to draw a straight line separating the two sets of data, and thereby create a model for classification.\n",
        "For two dimensional data like that shown here, this is a task we could do by hand.\n",
        "But immediately we see a problem: there is more than one possible dividing line that can perfectly discriminate between the two classes!\n",
        "\n",
        "We can draw them as follows:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UuBAXzJKJcAY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "xfit = np.linspace(-1, 3.5)\n",
        "plt.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='autumn')\n",
        "plt.plot([0.6], [2.1], 'x', color='red', markeredgewidth=2, markersize=10)\n",
        "\n",
        "for m, b in [(1, 0.65), (0.5, 1.6), (-0.2, 2.9)]:\n",
        "    plt.plot(xfit, m * xfit + b, '-k')\n",
        "\n",
        "plt.xlim(-1, 3.5);"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qErgyr-VJcAb",
        "colab_type": "text"
      },
      "source": [
        "These are three *very* different separators which, nevertheless, perfectly discriminate between these samples.\n",
        "Depending on which you choose, a new data point (e.g., the one marked by the \"X\" in this plot) will be assigned a different label!\n",
        "Evidently our simple intuition of \"drawing a line between classes\" is not enough, and we need to think a bit deeper."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Aqi8lCy6JcAc",
        "colab_type": "text"
      },
      "source": [
        "## Support Vector Machines: Maximizing the *Margin*\n",
        "\n",
        "Support vector machines offer one way to improve on this.\n",
        "The intuition is this: rather than simply drawing a zero-width line between the classes, we can draw around each line a *margin* of some width, up to the nearest point.\n",
        "Here is an example of how this might look:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Bz594DIJcAd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "xfit = np.linspace(-1, 3.5)\n",
        "plt.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='autumn')\n",
        "\n",
        "for m, b, d in [(1, 0.65, 0.33), (0.5, 1.6, 0.55), (-0.2, 2.9, 0.2)]:\n",
        "    yfit = m * xfit + b\n",
        "    plt.plot(xfit, yfit, '-k')\n",
        "    plt.fill_between(xfit, yfit - d, yfit + d, edgecolor='none',\n",
        "                     color='#AAAAAA', alpha=0.4)\n",
        "\n",
        "plt.xlim(-1, 3.5);"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZaH85hgyJcAh",
        "colab_type": "text"
      },
      "source": [
        "In support vector machines, the line that maximizes this margin is the one we will choose as the optimal model.\n",
        "Support vector machines are an example of such a *maximum margin* estimator."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pz6LRFxlJcAi",
        "colab_type": "text"
      },
      "source": [
        "### Fitting a support vector machine\n",
        "\n",
        "Let's see the result of an actual fit to this data: we will use Scikit-Learn's support vector classifier to train an SVM model on this data.\n",
        "For the time being, we will use a linear kernel and set the ``C`` parameter to a very large number (we'll discuss the meaning of these in more depth momentarily)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bCOc-k_xJcAj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.svm import SVC # \"Support vector classifier\"\n",
        "model = SVC(kernel='linear', C=1E10)\n",
        "model.fit(X, y)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RdTJTmg4JcAn",
        "colab_type": "text"
      },
      "source": [
        "To better visualize what's happening here, let's create a quick convenience function that will plot SVM decision boundaries for us:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EJMC3rP_JcAo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def plot_svc_decision_function(model, ax=None, plot_support=True):\n",
        "    \"\"\"Plot the decision function for a 2D SVC\"\"\"\n",
        "    if ax is None:\n",
        "        ax = plt.gca()\n",
        "    xlim = ax.get_xlim()\n",
        "    ylim = ax.get_ylim()\n",
        "    \n",
        "    # create grid to evaluate model\n",
        "    x = np.linspace(xlim[0], xlim[1], 30)\n",
        "    y = np.linspace(ylim[0], ylim[1], 30)\n",
        "    Y, X = np.meshgrid(y, x)\n",
        "    xy = np.vstack([X.ravel(), Y.ravel()]).T\n",
        "    P = model.decision_function(xy).reshape(X.shape)\n",
        "    \n",
        "    # plot decision boundary and margins\n",
        "    ax.contour(X, Y, P, colors='k',\n",
        "               levels=[-1, 0, 1], alpha=0.5,\n",
        "               linestyles=['--', '-', '--'])\n",
        "    \n",
        "    # plot support vectors\n",
        "    if plot_support:\n",
        "        ax.scatter(model.support_vectors_[:, 0],\n",
        "                   model.support_vectors_[:, 1],\n",
        "                   s=300, linewidth=1, facecolors='none');\n",
        "    ax.set_xlim(xlim)\n",
        "    ax.set_ylim(ylim)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4B83_FoyJcAr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='autumn')\n",
        "plot_svc_decision_function(model);"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DADxHnv-JcAu",
        "colab_type": "text"
      },
      "source": [
        "This is the dividing line that maximizes the margin between the two sets of points.\n",
        "Notice that a few of the training points just touch the margin: they are indicated by the black circles in this figure.\n",
        "These points are the pivotal elements of this fit, and are known as the *support vectors*, and give the algorithm its name.\n",
        "In Scikit-Learn, the identity of these points are stored in the ``support_vectors_`` attribute of the classifier:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Aimz6xKsJcAv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.support_vectors_"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rko1qBxlJcAy",
        "colab_type": "text"
      },
      "source": [
        "A key to this classifier's success is that for the fit, only the position of the support vectors matter; any points further from the margin which are on the correct side do not modify the fit!\n",
        "Technically, this is because these points do not contribute to the loss function used to fit the model, so their position and number do not matter so long as they do not cross the margin.\n",
        "\n",
        "We can see this, for example, if we plot the model learned from the first 60 points and first 120 points of this dataset:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WrbDVH_dJcAz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def plot_svm(N=10, ax=None):\n",
        "    X, y = make_blobs(n_samples=200, centers=2,\n",
        "                      random_state=0, cluster_std=0.60)\n",
        "    X = X[:N]\n",
        "    y = y[:N]\n",
        "    model = SVC(kernel='linear', C=1E10)\n",
        "    model.fit(X, y)\n",
        "    \n",
        "    ax = ax or plt.gca()\n",
        "    ax.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='autumn')\n",
        "    ax.set_xlim(-1, 4)\n",
        "    ax.set_ylim(-1, 6)\n",
        "    plot_svc_decision_function(model, ax)\n",
        "\n",
        "fig, ax = plt.subplots(1, 2, figsize=(16, 6))\n",
        "fig.subplots_adjust(left=0.0625, right=0.95, wspace=0.1)\n",
        "for axi, N in zip(ax, [60, 120]):\n",
        "    plot_svm(N, axi)\n",
        "    axi.set_title('N = {0}'.format(N))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mjp_8u_vJcA3",
        "colab_type": "text"
      },
      "source": [
        "In the left panel, we see the model and the support vectors for 60 training points.\n",
        "In the right panel, we have doubled the number of training points, but the model has not changed: the three support vectors from the left panel are still the support vectors from the right panel.\n",
        "This insensitivity to the exact behavior of distant points is one of the strengths of the SVM model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V8vdKialJcA_",
        "colab_type": "text"
      },
      "source": [
        "### Beyond linear boundaries: Kernel SVM\n",
        "\n",
        "Where SVM becomes extremely powerful is when it is combined with *kernels*.\n",
        "\n",
        "To motivate the need for kernels, let's look at some data that is not linearly separable:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5hmL_1zDJcBA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.datasets.samples_generator import make_circles\n",
        "X, y = make_circles(100, factor=.1, noise=.1)\n",
        "\n",
        "clf = SVC(kernel='linear').fit(X, y)\n",
        "\n",
        "plt.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='autumn')\n",
        "plot_svc_decision_function(clf, plot_support=False);"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GSRz1I7WJcBD",
        "colab_type": "text"
      },
      "source": [
        "It is clear that no linear discrimination will *ever* be able to separate this data. So now we have to think how we might project the data into a higher dimension such that a linear separator *would* be sufficient.\n",
        "\n",
        "For example, one simple projection we could use would be to compute a *radial basis function* centered on the middle clump:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9x0syBMiJcBE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "r = np.exp(-(X ** 2).sum(1))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KkmztFsxJcBH",
        "colab_type": "text"
      },
      "source": [
        "We can visualize this extra data dimension using a three-dimensional:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p_YYUQ1pJcBI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ax = plt.subplot(projection='3d')\n",
        "ax.scatter3D(X[:, 0], X[:, 1], r, c=y, s=50, cmap='autumn')\n",
        "ax.set_xlabel('x')\n",
        "ax.set_ylabel('y')\n",
        "ax.set_zlabel('r')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "46oIOcaHJcBK",
        "colab_type": "text"
      },
      "source": [
        "We can see that with this additional dimension, the data becomes trivially linearly separable, by drawing a separating plane at, say, *r*=0.7.\n",
        "\n",
        "Here we had to choose and carefully tune our projection: if we had not centered our radial basis function in the right location, we would not have seen such clean, linearly separable results.\n",
        "In general, the need to make such a choice is a problem: we would like to somehow automatically find the best basis functions to use.\n",
        "\n",
        "One strategy to this end is to compute a basis function centered at *every* point in the dataset, and let the SVM algorithm sift through the results.\n",
        "This type of basis function transformation is known as a *kernel transformation*, as it is based on a similarity relationship (or kernel) between each pair of points.\n",
        "\n",
        "A potential problem with this strategy—projecting $N$ points into $N$ dimensions—is that it might become very computationally intensive as $N$ grows large.\n",
        "However, because of a neat little procedure known as the [*kernel trick*](https://en.wikipedia.org/wiki/Kernel_trick), a fit on kernel-transformed data can be done implicitly—that is, without ever building the full $N$-dimensional representation of the kernel projection!\n",
        "This kernel trick is built into the SVM, and is one of the reasons the method is so powerful.\n",
        "\n",
        "In Scikit-Learn, we can apply kernelized SVM simply by changing our linear kernel to an RBF (radial basis function) kernel, using the ``kernel`` model hyperparameter:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4dvOe56pJcBL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "clf = SVC(kernel='rbf', C=1E6, gamma='auto')\n",
        "clf.fit(X, y)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mWaEUJjoJcBO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='autumn')\n",
        "plot_svc_decision_function(clf)\n",
        "plt.scatter(clf.support_vectors_[:, 0], clf.support_vectors_[:, 1],\n",
        "            s=300, lw=1, facecolors='none');"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5kDz9nCtJcBU",
        "colab_type": "text"
      },
      "source": [
        "Using this kernelized support vector machine, we learn a suitable nonlinear decision boundary.\n",
        "This kernel transformation strategy is used often in machine learning to turn fast linear methods into fast nonlinear methods, especially for models in which the kernel trick can be used."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KMegiEL_JcBe",
        "colab_type": "text"
      },
      "source": [
        "## Practical Example: Face Recognition\n",
        "\n",
        "As an example of support vector machines in action, let's take a look at the facial recognition problem.\n",
        "We will use the Labeled Faces in the Wild dataset, which consists of several thousand collated photos of various public figures.\n",
        "A fetcher for the dataset is built into Scikit-Learn:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DpGCtyTVJcBg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.datasets import fetch_lfw_people\n",
        "faces = fetch_lfw_people(min_faces_per_person=60)\n",
        "print(faces.target_names)\n",
        "print(faces.images.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EPGVcKhPJcBk",
        "colab_type": "text"
      },
      "source": [
        "Let's plot a few of these faces to see what we're working with:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xhKloIGDJcBl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "fig, ax = plt.subplots(3, 5)\n",
        "for i, axi in enumerate(ax.flat):\n",
        "    axi.imshow(faces.images[i], cmap='bone')\n",
        "    axi.set(xticks=[], yticks=[],\n",
        "            xlabel=faces.target_names[faces.target[i]])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cj27TYZVJcBo",
        "colab_type": "text"
      },
      "source": [
        "Each image contains [62×47] or nearly 3,000 pixels.\n",
        "We could proceed by simply using each pixel value as a feature, but often it is more effective to use some sort of preprocessor to extract more meaningful features; here we will use a principal component analysis (see [In Depth: Principal Component Analysis](05.09-Principal-Component-Analysis.ipynb)) to extract 150 fundamental components to feed into our support vector machine classifier.\n",
        "We can do this most straightforwardly by packaging the preprocessor and the classifier into a single pipeline:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xmLUrOwLJcBp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.svm import SVC\n",
        "from sklearn.decomposition import PCA as RandomizedPCA\n",
        "from sklearn.pipeline import make_pipeline\n",
        "\n",
        "pca = RandomizedPCA(n_components=150, whiten=True, random_state=42)\n",
        "svc = SVC(kernel='rbf', class_weight='balanced')\n",
        "model = make_pipeline(pca, svc)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XKFxekmNJcBt",
        "colab_type": "text"
      },
      "source": [
        "For the sake of testing our classifier output, we will split the data into a training and testing set:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bsa07DFnJcBu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "Xtrain, Xtest, ytrain, ytest = train_test_split(faces.data, faces.target,\n",
        "                                                random_state=42)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZkhjMeZdJcBx",
        "colab_type": "text"
      },
      "source": [
        "Finally, we can use a grid search cross-validation to explore combinations of parameters.\n",
        "Here we will adjust ``C`` (which controls the margin hardness) and ``gamma`` (which controls the size of the radial basis function kernel), and determine the best model:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uj6SmqgsJcBy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "param_grid = {'svc__C': [1, 5, 10, 50],\n",
        "              'svc__gamma': [0.0001, 0.0005, 0.001, 0.005]}\n",
        "grid = GridSearchCV(model, param_grid, cv=5)\n",
        "\n",
        "%time grid.fit(Xtrain, ytrain)\n",
        "print(grid.best_params_)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rh-gbWJwJcB4",
        "colab_type": "text"
      },
      "source": [
        "The optimal values fall toward the middle of our grid; if they fell at the edges, we would want to expand the grid to make sure we have found the true optimum.\n",
        "\n",
        "Now with this cross-validated model, we can predict the labels for the test data, which the model has not yet seen:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Aqm5OdpPJcB5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = grid.best_estimator_\n",
        "yfit = model.predict(Xtest)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V91Za4HrJcB-",
        "colab_type": "text"
      },
      "source": [
        "Let's take a look at a few of the test images along with their predicted values:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W0tUu-2qJcCA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "fig, ax = plt.subplots(4, 6)\n",
        "for i, axi in enumerate(ax.flat):\n",
        "    axi.imshow(Xtest[i].reshape(62, 47), cmap='bone')\n",
        "    axi.set(xticks=[], yticks=[])\n",
        "    axi.set_ylabel(faces.target_names[yfit[i]].split()[-1],\n",
        "                   color='black' if yfit[i] == ytest[i] else 'red')\n",
        "fig.suptitle('Predicted Names; Incorrect Labels in Red', size=14);"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qbfG7UX8JcCD",
        "colab_type": "text"
      },
      "source": [
        "Out of this small sample, our optimal estimator mislabeled only a single face (Bush’s\n",
        "face in the bottom row was mislabeled as Blair).\n",
        "We can get a better sense of our estimator's performance using the classification report, which lists recovery statistics label by label:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kmnDNsSzJcCE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.metrics import classification_report\n",
        "print(classification_report(ytest, yfit,\n",
        "                            target_names=faces.target_names))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WI1t4TrjJcCH",
        "colab_type": "text"
      },
      "source": [
        "We might also display the confusion matrix between these classes:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Je5YiLkIJcCI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "mat = confusion_matrix(ytest, yfit)\n",
        "sns.heatmap(mat.T, square=True, annot=True, fmt='d', cbar=False,\n",
        "            xticklabels=faces.target_names,\n",
        "            yticklabels=faces.target_names)\n",
        "plt.xlabel('true label')\n",
        "plt.ylabel('predicted label');"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KeZ5Ib5dJcCM",
        "colab_type": "text"
      },
      "source": [
        "This helps us get a sense of which labels are likely to be confused by the estimator.\n",
        "\n",
        "For a real-world facial recognition task, in which the photos do not come pre-cropped into nice grids, the only difference in the facial classification scheme is the feature selection: you would need to use a more sophisticated algorithm to find the faces, and extract features that are independent of the pixellation.\n",
        "For this kind of application, one good option is to make use of [OpenCV](http://opencv.org), which, among other things, includes pre-trained implementations of state-of-the-art feature extraction tools for images in general and faces in particular."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "crDt2C9VJcCN",
        "colab_type": "text"
      },
      "source": [
        "## Support Vector Machine Summary\n",
        "\n",
        "We have seen here a brief intuitive introduction to the principals behind support vector machines.\n",
        "These methods are a powerful classification method for a number of reasons:\n",
        "\n",
        "- Their dependence on relatively few support vectors means that they are very compact models, and take up very little memory.\n",
        "- Once the model is trained, the prediction phase is very fast.\n",
        "- Because they are affected only by points near the margin, they work well with high-dimensional data—even data with more dimensions than samples, which is a challenging regime for other algorithms.\n",
        "- Their integration with kernel methods makes them very versatile, able to adapt to many types of data.\n",
        "\n",
        "However, SVMs have several disadvantages as well:\n",
        "\n",
        "- The scaling with the number of samples $N$ is $\\mathcal{O}[N^3]$ at worst, or $\\mathcal{O}[N^2]$ for efficient implementations. For large numbers of training samples, this computational cost can be prohibitive.\n",
        "- The results are strongly dependent on a suitable choice for the softening parameter $C$. This must be carefully chosen via cross-validation, which can be expensive as datasets grow in size.\n",
        "- The results do not have a direct probabilistic interpretation. This can be estimated via an internal cross-validation (see the ``probability`` parameter of ``SVC``), but this extra estimation is costly.\n",
        "\n",
        "With those traits in mind, I generally only turn to SVMs once other simpler, faster, and less tuning-intensive methods have been shown to be insufficient for my needs.\n",
        "Nevertheless, if you have the CPU cycles to commit to training and cross-validating an SVM on your data, the method can lead to excellent results."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4UGJm5zfKD5k",
        "colab_type": "text"
      },
      "source": [
        "# Sources:\n",
        "\n",
        "https://stackabuse.com/association-rule-mining-via-apriori-algorithm-in-python/\n",
        "\n",
        "https://medium.com/@kbrook10/day-11-machine-learning-using-knn-k-nearest-neighbors-with-scikit-learn-350c3a1402e6\n",
        "\n",
        "This notebook contains an excerpt from the [Python Data Science Handbook](http://shop.oreilly.com/product/0636920034919.do) by Jake VanderPlas"
      ]
    }
  ]
}