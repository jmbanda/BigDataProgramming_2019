{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Chapter 5 - Loading and Saving Data in Spark.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jmbanda/BigDataProgramming_2019/blob/master/Chapter_5_Loading_and_Saving_Data_in_Spark.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZD1zvV9JtRsN",
        "colab_type": "text"
      },
      "source": [
        "# Loading and Saving Data in Spark"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BXcc49lmUYgz",
        "colab_type": "text"
      },
      "source": [
        "Collab Only code:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L_U5TbtAUX_r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!wget -q https://www-us.apache.org/dist/spark/spark-2.4.4/spark-2.4.4-bin-hadoop2.7.tgz\n",
        "!tar xf spark-2.4.4-bin-hadoop2.7.tgz\n",
        "!pip install -q findspark\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-2.4.4-bin-hadoop2.7\"\n",
        "import findspark\n",
        "findspark.init()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KbRIu4xkU2qN",
        "colab_type": "text"
      },
      "source": [
        "**Not on Colab you should start form HERE:**\n",
        "\n",
        "Reading a text file textFile() in Python"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yMHp9gMPUyPr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder \\\n",
        "    .master(\"local[*]\") \\\n",
        "    .appName(\"Learning_Spark\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "sc = spark.sparkContext\n",
        "lines = sc.textFile(\"spark-2.4.4-bin-hadoop2.7/README.md\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uPoJs6cguwyh",
        "colab_type": "text"
      },
      "source": [
        "Loading all the .md files in one directory"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K3ccsKyvu0qT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "input = sc.textFile(\"spark-2.4.4-bin-hadoop2.7/*.md\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5f8uBm53VM1H",
        "colab_type": "text"
      },
      "source": [
        "**Only in Google Colab:**\n",
        "\n",
        "Load the example1.json JSON file (found on iCollege under Datasets)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jeQp5sG3VKqR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import files\n",
        "files.upload()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WX8TQit5xbYU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "jsonDF = spark.read.json('example1.json')\n",
        "jsonDF.printSchema()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TQ7cLUwd1AbP",
        "colab_type": "text"
      },
      "source": [
        "# Saving Text Files\n",
        "\n",
        "Using the USDA_activity_dataset_csv dataset (found on iCollege under Datasets)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "N9_EjG_h1Q5R"
      },
      "source": [
        "**Only in Google Colab:**\n",
        "\n",
        "Load the USDA file from Disk. \n",
        "\n",
        "NOTEL: Convert it to CSV on Excel first! \n",
        "\n",
        "Note: You might have to run this twice so it works fine."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "rvb7iQbv1Q5S",
        "colab": {}
      },
      "source": [
        "from google.colab import files\n",
        "files.upload()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j9w4g1vh1e8j",
        "colab_type": "text"
      },
      "source": [
        "**Reading a CSV file into a DataFrame, filter some columns and save it**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GuKbsrvUVhse",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data = spark.read.csv('USDA_activity_dataset_csv.csv',inferSchema=True, header=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3ICHjQoP1hrz",
        "colab_type": "text"
      },
      "source": [
        "Filter data by several columns"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2hXyJNFVWjux",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dataF=data.select(\"State\",\"County\",\"Median household income\",\"Poverty rate\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "39l8vOAA18x9",
        "colab_type": "text"
      },
      "source": [
        "Save only the filtered Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xxlQXKjg11mM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dataF.write.csv(\"USDA_income_poverty.csv\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i1cg9iM93qCg",
        "colab_type": "text"
      },
      "source": [
        "Let's read this new file back into an RDD"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v-iSzucQ3sup",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "rddL=sc.textFile(\"USDA_income_poverty.csv\")\n",
        "rddL.collect()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6l9c1Jdp5O6c",
        "colab_type": "text"
      },
      "source": [
        "# **Hive Example**\n",
        "\n",
        "Using Hive to create and read a table - Simple Example"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lOcCViaN4cwC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from pyspark.sql import Row\n",
        "from pyspark.sql import HiveContext\n",
        "sqlContext = HiveContext(sc)\n",
        "test_list = [('A', 25),('B', 20),('C', 25),('D', 18)]\n",
        "rdd = sc.parallelize(test_list)\n",
        "people = rdd.map(lambda x: Row(name=x[0], age=int(x[1])))\n",
        "schemaPeople = sqlContext.createDataFrame(people)\n",
        "# Register it as a temp table\n",
        "sqlContext.registerDataFrameAsTable(schemaPeople, \"test_table\")\n",
        "sqlContext.sql(\"show tables\").show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jWcwTJx_5cYW",
        "colab_type": "text"
      },
      "source": [
        "Let's query the table"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uaX5OJaz5eAz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sqlContext.sql(\"Select * from test_table\").show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zIcPB7Mi5-Zu",
        "colab_type": "text"
      },
      "source": [
        "**Load a JSON file with Hive and use SQL on it**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SKD0QGQn6G3H",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Colab code only - DO NOT run outsie of colab\n",
        "from google.colab import files  \n",
        "files.upload()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5J7JUJy16OJF",
        "colab_type": "text"
      },
      "source": [
        "Let's load example1.json with Hive a do a Select Statement on it"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "axQ3wgQS6aLM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from pyspark.sql import HiveContext\n",
        "hiveCtx = HiveContext(sc)\n",
        "ex1 = hiveCtx.read.json(\"example1.json\")\n",
        "ex1.registerTempTable(\"ex1\")\n",
        "results = hiveCtx.sql(\"SELECT ename, sal FROM ex1\").show()"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}