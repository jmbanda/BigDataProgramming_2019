{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Chapter4-Working with Key/Value Pairs.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jmbanda/BigDataProgramming_2019/blob/master/Chapter4_Working_with_Key_Value_Pairs.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QMX89L1ZUbSb",
        "colab_type": "text"
      },
      "source": [
        "# Working with Key/Value Pairs\n",
        "\n",
        "Key/value RDDs are commonly used to perform aggregations, and often we will do some initial ETL (extract, transform, and load) to get our data into a key/value format. Key/value RDDs expose new operations (e.g., counting up reviews for each product, grouping together data with the same key, and grouping together two different RDDs)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BXcc49lmUYgz",
        "colab_type": "text"
      },
      "source": [
        "Collab Only code:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L_U5TbtAUX_r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!wget -q https://www-us.apache.org/dist/spark/spark-2.4.4/spark-2.4.4-bin-hadoop2.7.tgz\n",
        "!tar xf spark-2.4.4-bin-hadoop2.7.tgz\n",
        "!pip install -q findspark\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-2.4.4-bin-hadoop2.7\"\n",
        "import findspark\n",
        "findspark.init()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KbRIu4xkU2qN",
        "colab_type": "text"
      },
      "source": [
        "**Not on Colab you should start form HERE:**\n",
        "\n",
        "Creating an RDD with textFile() in Python"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yMHp9gMPUyPr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder \\\n",
        "    .master(\"local[*]\") \\\n",
        "    .appName(\"Learning_Spark\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "sc = spark.sparkContext\n",
        "lines = sc.textFile(\"spark-2.4.4-bin-hadoop2.7/README.md\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dy3PZo2YVR91",
        "colab_type": "text"
      },
      "source": [
        "Creating a pair RDD using the first word as the key:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SKid0J1oVS_d",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pairs = lines.map(lambda x: (x.split(\" \")[0], x))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "enYsEqTHWCd3",
        "colab_type": "text"
      },
      "source": [
        "Let's see how our Key/Value pair looks like:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P09SQLJzWFTB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(pairs.collect())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ertUaWnX1OH",
        "colab_type": "text"
      },
      "source": [
        "# Transformations on pair RDDs\n",
        "\n",
        "Lets use  {(1, 2), (3, 4), (3, 6)}  as our example."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V9QOapnDaHnp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "exampleRDDt = sc.parallelize([(1, 2), (3, 4), (3, 6)])\n",
        "exampleRDD = exampleRDDt.map(lambda x: (int(x[0]), int(x[1])))   # What is this doing?\n",
        "print(exampleRDD.collect())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NkIqwhpEeID7",
        "colab_type": "text"
      },
      "source": [
        "***reduceByKey(func)***\n",
        "\n",
        "Combine values with the same key."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x6MZKGCDeJ3f",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "rbK = exampleRDD.reduceByKey(lambda x, y: x + y)\n",
        "print(rbK.collect())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r1bO6Pyre_Fr",
        "colab_type": "text"
      },
      "source": [
        "***groupByKey()***\n",
        "\n",
        "Group values with the same key."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uaE8EfjjfBfY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "exampleRDD.groupByKey().collect()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xaQt4BQkfjMX",
        "colab_type": "text"
      },
      "source": [
        "How do we actually see the elements?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MkZ-z3PyfbJV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "exampleRDD.groupByKey().map(lambda x : (x[0], list(x[1]))).collect()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KNlGZYOIf39N",
        "colab_type": "text"
      },
      "source": [
        "***mapValues(func)***\n",
        "\n",
        "Apply a function to each value of a pair RDD without changing the key.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gw5rhd0Uf5j8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "exampleRDD.mapValues(lambda x: x+1).collect()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gKVL5kWyiBj6",
        "colab_type": "text"
      },
      "source": [
        "***flatMapValues(func)***\n",
        "\n",
        "Apply a function that returns an iterator to each value of a pair RDD, and for each element returned, produce a key/value entry with the old key. Often used for tokenization."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c1r1cC-kiLOx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "exampleRDD.flatMapValues(lambda x: range(x,6)).collect()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1p_iM268jQpP",
        "colab_type": "text"
      },
      "source": [
        "***keys()***\n",
        "\n",
        "Return an RDD of just the keys."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LSdn0rUejSuF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "exampleRDD.keys().collect()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GOs2wHQEjhzO",
        "colab_type": "text"
      },
      "source": [
        "***values()***\n",
        "\n",
        "Return an RDD of just the values."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wtGohJH8jlbO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "exampleRDD.values().collect()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Zky0-sPj9no",
        "colab_type": "text"
      },
      "source": [
        "***sortByKey()***\n",
        "\n",
        "Return an RDD sorted by the key."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TjS4ZL0llI-k",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "exampleRDD.sortByKey().collect()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cYj_YGRrXJay",
        "colab_type": "text"
      },
      "source": [
        "***Go back to slides!!!***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IWYzeku2mvqt",
        "colab_type": "text"
      },
      "source": [
        "# Transformations on two pair RDDs \n",
        "\n",
        "rdd = {(1, 2), (3, 4), (3, 6)} \n",
        "other = {(3, 9)})"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P-yxwkHEmuyE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "exampleRDDt = sc.parallelize([(1, 2), (3, 4), (3, 6)])\n",
        "rdd = exampleRDDt.map(lambda x: (int(x[0]), int(x[1])))  \n",
        "\n",
        "othert = sc.parallelize([(3, 9)])\n",
        "other = othert.map(lambda x: (int(x[0]), int(x[1])))  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tpCWILv3nURr",
        "colab_type": "text"
      },
      "source": [
        "***subtractByKey***\n",
        "\n",
        "Remove elements with a key present in the other RDD."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ddAjf-q1nYzf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "rdd.subtractByKey(other).collect()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3bzxv7QLoNz_",
        "colab_type": "text"
      },
      "source": [
        "***join***\n",
        "\n",
        "Perform an inner join between two RDDs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oYzKs82coREB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "rdd.join(other).collect()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VgxmSpyAoYxC",
        "colab_type": "text"
      },
      "source": [
        "***rightOuterJoin***\n",
        "\n",
        "Perform a join between two RDDs where the key must be present in the first RDD."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o_YcfWAlocpL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "rdd.rightOuterJoin(other).collect()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7JgQlog5vMOe",
        "colab_type": "text"
      },
      "source": [
        "***leftOuterJoin***\n",
        "\n",
        "Perform a join between two RDDs where the key must be present in the other RDD."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DQAV5d-OvRqd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "rdd.leftOuterJoin(other).collect()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2VaGsueBvOwS",
        "colab_type": "text"
      },
      "source": [
        "***cogroup***\n",
        "\n",
        "Group data from both RDDs sharing the same key."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sE9RTKCow_-R",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "rdd.cogroup(other).collect()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nf4MliIvxhEz",
        "colab_type": "text"
      },
      "source": [
        "Pair RDDs are also still RDDs (of Python tuples), and thus support the same functions as RDDs. For instance, we can take our pair RDD from the previous section and filter out lines longer than 20 characters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uLN7Xlwwxn6r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "result = pairs.filter(lambda keyValue: len(keyValue[1]) < 20)\n",
        "result.collect()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "at0HV7uzXujV",
        "colab_type": "text"
      },
      "source": [
        "***Back to Slides***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ybc10cRr5-ze",
        "colab_type": "text"
      },
      "source": [
        "# WordCount Example\n",
        "\n",
        "Using the file: https://norvig.com/big.txt\n",
        "\n",
        "Lets do the following tasks:\n",
        "\n",
        "1) Open read a text file from the web. NOTE: I know the encoding is UTF-8, but you might have to check like it is recommended here (https://stackoverflow.com/questions/37369901/attributeerror-httpresponse-object-has-no-attribute-split)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "03JL8dXO6yQ_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import urllib.request\n",
        "data = urllib.request.urlopen(\"https://norvig.com/big.txt\").read().decode('utf-8')\n",
        "data = data.split(\"\\n\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s1-I4QCR791r",
        "colab_type": "text"
      },
      "source": [
        "Now that we have the file by lines, let's do a word count:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5ByvLjzV8Cu3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "words = sc.parallelize(data).flatMap(lambda line: line.split(\" \"))\n",
        "wordCounts = words.map(lambda word: (word, 1)).reduceByKey(lambda a,b:a +b)\n",
        "print(wordCounts.collect())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pVpFPVrP82qW",
        "colab_type": "text"
      },
      "source": [
        "Is there another way?\n",
        "\n",
        "Yes, and it is faster!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UxvvhgrA84OB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "result = words.flatMap(lambda x: x.split(\" \")).countByValue()\n",
        "print(result)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vUXgcbTLAio0",
        "colab_type": "text"
      },
      "source": [
        "**BACK TO SLIDES**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WgzIvUSNEoOH",
        "colab_type": "text"
      },
      "source": [
        "# combineByKey"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q2G-DatFAnzD",
        "colab_type": "text"
      },
      "source": [
        "In order to aggregate an RDD’s elements in parallel, Spark’s ***combineByKey*** method requires three functions:\n",
        "\n",
        "```\n",
        "createCombiner\n",
        "mergeValue\n",
        "mergeCombiner\n",
        "```\n",
        "\n",
        "**Creating a Combiner:**\n",
        "\n",
        "`lambda value: (value, 1)`\n",
        "\n",
        "The first required argument in the ***combineByKey*** method is a function to be used as the very first aggregation step for each key. The argument of this function corresponds to the value in a key-value pair. If we want to compute the `sum` and `count` using ***combineByKey***, then we can create this “combiner” to be a tuple in the form of `(sum, count)`. The very first step in this aggregation is then `(value, 1)`, where `value` is the first RDD value that ***combineByKey*** comes across and `1` initializes the count.\n",
        "\n",
        "**Merge a Value:**\n",
        "\n",
        "`lambda x, value: (x[0] + value, x[1] + 1)`\n",
        "\n",
        "The next required function tells ***combineByKey*** what to do when a combiner is given a new value. The arguments to this function are a combiner and a new value. The structure of the combiner is defined above as a tuple in the form of `(sum, count)` so we merge the new value by adding it to the first element of the tuple while incrementing `1` to the second element of the tuple.\n",
        "\n",
        "**Merge two Combiners:**\n",
        "\n",
        "`lambda x, y: (x[0] + y[0], x[1] + y[1])`\n",
        "\n",
        "The final required function tells ***combineByKey*** how to merge two combiners. In this example with tuples as combiners in the form of `(sum, count)`, all we need to do is add the first and last elements together."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "02tLDcwoOx8V",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "student_rdd = sc.parallelize([\n",
        "  (\"Joseph\", \"Maths\", 83), (\"Joseph\", \"Physics\", 74), (\"Joseph\", \"Chemistry\", 91), \n",
        "    (\"Joseph\", \"Biology\", 82), (\"Jimmy\", \"Maths\", 69), (\"Jimmy\", \"Physics\", 62), \n",
        "    (\"Jimmy\", \"Chemistry\", 97), (\"Jimmy\", \"Biology\", 80), (\"Tina\", \"Maths\", 78), \n",
        "    (\"Tina\", \"Physics\", 73), (\"Tina\", \"Chemistry\", 68), (\"Tina\", \"Biology\", 87), \n",
        "    (\"Thomas\", \"Maths\", 87), (\"Thomas\", \"Physics\", 93), (\"Thomas\", \"Chemistry\", 91), \n",
        "    (\"Thomas\", \"Biology\", 74), (\"Cory\", \"Maths\", 56), (\"Cory\", \"Physics\", 65), \n",
        "    (\"Cory\", \"Chemistry\", 71), (\"Cory\", \"Biology\", 68), (\"Jackeline\", \"Maths\", 86), \n",
        "    (\"Jackeline\", \"Physics\", 62), (\"Jackeline\", \"Chemistry\", 75), (\"Jackeline\", \"Biology\", 83), \n",
        "    (\"Juan\", \"Maths\", 63), (\"Juan\", \"Physics\", 69), (\"Juan\", \"Chemistry\", 64), \n",
        "    (\"Juan\", \"Biology\", 60)], 3)\n",
        " \n",
        "# Defining createCombiner, mergeValue and mergeCombiner functions\n",
        "def createCombiner(tpl):\n",
        "    return (tpl[1], 1)\n",
        "    \n",
        "def mergeValue(accumulator, element): \n",
        "    return (accumulator[0] + element[1], accumulator[1] + 1)\n",
        "    \n",
        "def mergeCombiner(accumulator1, accumulator2): \n",
        "    return (accumulator1[0] + accumulator2[0], accumulator1[1] + accumulator2[1])\n",
        " \n",
        "comb_rdd = student_rdd.map(lambda t: (t[0], (t[1], t[2]))) \\\n",
        "                    .combineByKey(createCombiner, mergeValue, mergeCombiner) \\\n",
        "                    .map(lambda t: (t[0], t[1][0]/t[1][1]))\n",
        " \n",
        "# See output nicely\n",
        "for tpl in comb_rdd.collect():\n",
        "    print(tpl)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yl4pNf_1Pm8z",
        "colab_type": "text"
      },
      "source": [
        "**Important Points**\n",
        "\n",
        "* Apache spark ***combineByKey*** is a transformation operation hence its evaluation is lazy\n",
        "* It is a wider operation as it shuffles data in the last stage of aggregation and creates another RDD\n",
        "* Recommended to use when you need to do further aggregation on grouped data\n",
        "* Use ***combineByKey*** when return type differs than source type (i.e. when you cannot use ***reduceByKey*** )\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tFpazTMyQlWN",
        "colab_type": "text"
      },
      "source": [
        "**Got back to Slides!**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QT24fHk-QdEW",
        "colab_type": "text"
      },
      "source": [
        "# **reduceByKey()** with custom parallelism"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jevyVIhMQuR9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dataF = [(\"a\", 3), (\"b\", 4), (\"a\", 1)]\n",
        "sample1 = sc.parallelize(dataF).reduceByKey(lambda x,y: x + y) # Default parallelism\n",
        "sample2 = sc.parallelize(dataF).reduceByKey(lambda x,y: x + y, 10) # Custom parallelism\n",
        "\n",
        "print(sample1.collect())\n",
        "print(sample2.collect())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5ot29TNOR6yj",
        "colab_type": "text"
      },
      "source": [
        "# Joins\n",
        "\n",
        "***Innerjoin*** example.\n",
        "\n",
        "The simple join operator is an inner join. Only keys that are present in both pair RDDs are output. When there are multiple values for the same key in one of the inputs, the resulting pair RDD will have an entry for every possible pair of values with that key from the two input RDDs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7KkEc1vJSBp5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "names1 = sc.parallelize((\"abe\", \"abby\", \"apple\")).map(lambda a: (a, 1))\n",
        "names2 = sc.parallelize((\"apple\", \"beatty\", \"beatrice\")).map(lambda a: (a, 1))\n",
        "names1.join(names2).collect()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AxAHsMjLS0Wb",
        "colab_type": "text"
      },
      "source": [
        "Sometimes we don’t need the key to be present in both RDDs to want it in our result. For example, if we were joining customer information with recommendations we might not want to drop customers if there were not any recommendations yet. `leftOuterJoin(other)` and `rightOuterJoin(other)` both join pair RDDs together by key, where one of the pair RDDs can be missing the key."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OYgcBpKQTLnE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "names1.leftOuterJoin(names2).collect()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KGd6szy9TU9M",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "names1.rightOuterJoin(names2).collect()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TqH4Cl5hX5bi",
        "colab_type": "text"
      },
      "source": [
        "**Go back to slides**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VR9vr8s0Usin",
        "colab_type": "text"
      },
      "source": [
        "# **Sorting Data**\n",
        "\n",
        "Syntax: **sortByKey(*ascending=True,numPartitions=None,keyfunc = lambda x:str(x)*)**\n",
        "\n",
        "Sort and show first element\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8rXLZe-iU0Po",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tmp = [('a', 1), ('b', 2), ('1', 3), ('d', 4), ('2', 5)]\n",
        "sc.parallelize(tmp).sortByKey().first()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5QzBMP8OVEzc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sc.parallelize(tmp).sortByKey(True, 1).collect()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VXEYFhVwVIWB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sc.parallelize(tmp).sortByKey(True, 2).collect()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Myggkc2VdVV",
        "colab_type": "text"
      },
      "source": [
        "Another example, look at the usages of extend"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d4SDVQ47VfyJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tmp2 = [('Mary', 1), ('had', 2), ('a', 3), ('little', 4), ('lamb', 5)]\n",
        "tmp2.extend([('whose', 6), ('fleece', 7), ('was', 8), ('white', 9)])\n",
        "sc.parallelize(tmp2).sortByKey(True, 3, keyfunc=lambda k: k.lower()).collect()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-aAepI6BfoMX",
        "colab_type": "text"
      },
      "source": [
        "# Additional Examples"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FEipT6CDZkBf",
        "colab_type": "text"
      },
      "source": [
        "**Quick Example**: Count all words (naively) from the README.MD file:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e1C5knrgX0dc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "counts = lines.flatMap(lambda line: line.split(\" \")). \\\n",
        "             map(lambda word: (word, 1)). \\\n",
        "             reduceByKey(lambda a, b: a + b)\n",
        "\n",
        "print(counts.collect())\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r_QyaRj3Un45",
        "colab_type": "text"
      },
      "source": [
        "Sources:\n",
        "\n",
        "https://www.oreilly.com/library/view/learning-spark/9781449359034/ch04.html\n",
        "\n",
        "https://backtobazics.com/big-data/apache-spark-combinebykey-example/\n",
        "\n",
        "https://supergloo.com/spark-python/apache-spark-transformations-python-examples/"
      ]
    }
  ]
}